{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "nlp-project.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AKMADOU/GMM-Group-Project/blob/main/nlp_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4 as bs\n",
        "import urllib\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import re\n",
        "import heapq\n",
        "from string import punctuation"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2022-04-21T10:45:37.443303Z",
          "iopub.execute_input": "2022-04-21T10:45:37.443629Z",
          "iopub.status.idle": "2022-04-21T10:45:49.226816Z",
          "shell.execute_reply.started": "2022-04-21T10:45:37.443553Z",
          "shell.execute_reply": "2022-04-21T10:45:49.226031Z"
        },
        "trusted": true,
        "id": "5HW0lVl3L2Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-21T12:44:24.291584Z",
          "iopub.execute_input": "2022-04-21T12:44:24.291860Z",
          "iopub.status.idle": "2022-04-21T12:44:24.884953Z",
          "shell.execute_reply.started": "2022-04-21T12:44:24.291831Z",
          "shell.execute_reply": "2022-04-21T12:44:24.884202Z"
        },
        "trusted": true,
        "id": "aUC0EKKCL2Od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-21T12:44:25.309921Z",
          "iopub.execute_input": "2022-04-21T12:44:25.310469Z",
          "iopub.status.idle": "2022-04-21T12:44:25.319207Z",
          "shell.execute_reply.started": "2022-04-21T12:44:25.310435Z",
          "shell.execute_reply": "2022-04-21T12:44:25.318251Z"
        },
        "trusted": true,
        "id": "ZSeRhXDbL2Od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Columns\n",
        "\n",
        "The \"review\" column contains the textual information(input features) and the \"sentiment\" column contains the output labels. The task of any classifier is to correctly predict the \"sentiment\" given any \"review\" or textual column. Hence we have to apply our data cleaning, transformation steps to the \"review\" column."
      ],
      "metadata": {
        "id": "xdCo17dDL2Oe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Assess the shape of the data\n",
        "print(\"The Shape of the Dataset\".format(),train_df.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-21T12:44:26.709798Z",
          "iopub.execute_input": "2022-04-21T12:44:26.710472Z",
          "iopub.status.idle": "2022-04-21T12:44:26.714733Z",
          "shell.execute_reply.started": "2022-04-21T12:44:26.710435Z",
          "shell.execute_reply": "2022-04-21T12:44:26.714008Z"
        },
        "trusted": true,
        "id": "d7bmAmpTL2Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This is the start of the analysis phase where we will first check the amount of data present in either of the sentiments. We will follow this up with some pictorial representations related to the words and frequency mappings."
      ],
      "metadata": {
        "id": "9HPMECPjL2Of"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "good_reviews=train_df[train_df['sentiment']=='positive']['review']\n",
        "bad_reviews=train_df[train_df['sentiment']=='negative']['review']\n",
        "print(\"First 10 samples of good reviews\\n\".format(),good_reviews[:10])\n",
        "print(\"First 10 samples of bad reviews\\n\".format(),bad_reviews[:10])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-21T12:44:27.439964Z",
          "iopub.execute_input": "2022-04-21T12:44:27.440526Z",
          "iopub.status.idle": "2022-04-21T12:44:27.484998Z",
          "shell.execute_reply.started": "2022-04-21T12:44:27.440483Z",
          "shell.execute_reply": "2022-04-21T12:44:27.484041Z"
        },
        "trusted": true,
        "id": "UEyxIzH6L2Of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Count of good and bad reviews\n",
        "count=train_df['sentiment'].value_counts()\n",
        "print('Total Counts of both sets'.format(),count)\n",
        "\n",
        "print(\"==============\")\n",
        "#Creating a function to plot the counts using matplotlib\n",
        "def plot_counts(count_good,count_bad):\n",
        "    plt.rcParams['figure.figsize']=(6,6)\n",
        "    plt.bar(0,count_good,width=0.6,label='Positive Reviews',color='Green')\n",
        "    plt.legend()\n",
        "    plt.bar(2,count_bad,width=0.6,label='Negative Reviews',color='Red')\n",
        "    plt.legend()\n",
        "    plt.ylabel('Count of Reviews')\n",
        "    plt.xlabel('Types of Reviews')\n",
        "    plt.show()\n",
        "    \n",
        "count_good=train_df[train_df['sentiment']=='positive']\n",
        "count_bad=train_df[train_df['sentiment']=='negative']\n",
        "plot_counts(len(count_good),len(count_bad))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-21T12:44:27.779981Z",
          "iopub.execute_input": "2022-04-21T12:44:27.780883Z",
          "iopub.status.idle": "2022-04-21T12:44:27.989596Z",
          "shell.execute_reply.started": "2022-04-21T12:44:27.780844Z",
          "shell.execute_reply": "2022-04-21T12:44:27.988858Z"
        },
        "trusted": true,
        "id": "PnuM-SBjL2Og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_df),train_df.index.shape[-1]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-21T12:44:28.265388Z",
          "iopub.execute_input": "2022-04-21T12:44:28.265899Z",
          "iopub.status.idle": "2022-04-21T12:44:28.271841Z",
          "shell.execute_reply.started": "2022-04-21T12:44:28.265854Z",
          "shell.execute_reply": "2022-04-21T12:44:28.271139Z"
        },
        "trusted": true,
        "id": "4zjCERbTL2Og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Analyse the count of words in each segment- both positive and negative reviews\n",
        "#Function for checking word length\n",
        "def cal_len(data):\n",
        "    return len(data)\n",
        "\n",
        "#Create generic plotter with Seaborn\n",
        "def plot_count(count_ones,count_zeros,title_1,title_2,subtitle):\n",
        "    fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,5))\n",
        "    sns.distplot(count_zeros,ax=ax1,color='Blue')\n",
        "    ax1.set_title(title_1)\n",
        "    sns.distplot(count_ones,ax=ax2,color='Red')\n",
        "    ax2.set_title(title_2)\n",
        "    fig.suptitle(subtitle)\n",
        "    plt.show()    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "count_good_words=count_good['review'].str.split().apply(lambda z:cal_len(z))\n",
        "count_bad_words=count_bad['review'].str.split().apply(lambda z:cal_len(z))\n",
        "print(\"Positive Review Words:\" + str(count_good_words))\n",
        "print(\"Negative Review Words:\" + str(count_bad_words))\n",
        "plot_count(count_good_words,count_bad_words,\"Positive Review\",\"Negative Review\",\"Reviews Word Analysis\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-21T12:44:28.831289Z",
          "iopub.execute_input": "2022-04-21T12:44:28.833870Z",
          "iopub.status.idle": "2022-04-21T12:44:31.468820Z",
          "shell.execute_reply.started": "2022-04-21T12:44:28.833824Z",
          "shell.execute_reply": "2022-04-21T12:44:31.468125Z"
        },
        "trusted": true,
        "id": "ePyFKQVcL2Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Analyse Stopwords\n",
        "\n",
        "def plot_count_1(count_ones,count_zeros,title_1,title_2,subtitle):\n",
        "    fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,5))\n",
        "    sns.distplot(count_zeros,ax=ax1,color='Blue')\n",
        "    ax1.set_title(title_1)\n",
        "    sns.distplot(count_ones,ax=ax2,color='Orange')\n",
        "    ax2.set_title(title_2)\n",
        "    fig.suptitle(subtitle)\n",
        "    plt.show()    \n",
        "\n",
        "\n",
        "stops=set(stopwords.words('english'))\n",
        "count_good_stops=count_good['review'].apply(lambda z : np.mean([len(z) for w in str(z).split()]))\n",
        "count_bad_stops=count_bad['review'].apply(lambda z : np.mean([len(z) for w in str(z).split()]))\n",
        "plot_count_1(count_good_stops,count_bad_stops,\"Positive Reviews Stopwords\",\"Negative Reviews Stopwords\",\"Reviews Stopwords Analysis\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-21T12:44:31.470386Z",
          "iopub.execute_input": "2022-04-21T12:44:31.471360Z",
          "iopub.status.idle": "2022-04-21T12:44:31.501239Z",
          "shell.execute_reply.started": "2022-04-21T12:44:31.471318Z",
          "shell.execute_reply": "2022-04-21T12:44:31.500347Z"
        },
        "trusted": true,
        "id": "68gjGKLNL2Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Checking number of Urls\n",
        "count_good_urls=count_good['review'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
        "count_bad_urls=count_bad['review'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
        "\n",
        "plot_count_1(count_good_stops,count_bad_stops,\"Positive Reviews URLs\",\"Negative Reviews URLs\",\"Reviews URLs Analysis\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-21T12:44:31.502240Z",
          "iopub.status.idle": "2022-04-21T12:44:31.504786Z",
          "shell.execute_reply.started": "2022-04-21T12:44:31.504261Z",
          "shell.execute_reply": "2022-04-21T12:44:31.504289Z"
        },
        "trusted": true,
        "id": "P-upATumL2Oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,46):\n",
        "    article_text = train_df.replace(str([i]), \"\")\n",
        "print(article_text)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-21T12:44:31.506246Z",
          "iopub.status.idle": "2022-04-21T12:44:31.506683Z",
          "shell.execute_reply.started": "2022-04-21T12:44:31.506447Z",
          "shell.execute_reply": "2022-04-21T12:44:31.506471Z"
        },
        "trusted": true,
        "id": "nR_tnYAoL2Oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-21T12:44:31.904631Z",
          "iopub.execute_input": "2022-04-21T12:44:31.904904Z",
          "iopub.status.idle": "2022-04-21T12:44:40.528143Z",
          "shell.execute_reply.started": "2022-04-21T12:44:31.904876Z",
          "shell.execute_reply": "2022-04-21T12:44:40.527045Z"
        },
        "trusted": true,
        "id": "9hc6XNaUL2Oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    row = re.sub(r\"\\\\\", \" \" ,text, flags=re.DOTALL)\n",
        "    row = re.sub(r\"\\([^()]*\\)\", \" \", row,flags=re.DOTALL) \n",
        "    row = re.sub(r\"\\n\", \" \", row,flags=re.DOTALL)\n",
        "    return row"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-21T12:44:40.530917Z",
          "iopub.execute_input": "2022-04-21T12:44:40.531224Z",
          "iopub.status.idle": "2022-04-21T12:44:40.536378Z",
          "shell.execute_reply.started": "2022-04-21T12:44:40.531171Z",
          "shell.execute_reply": "2022-04-21T12:44:40.535662Z"
        },
        "trusted": true,
        "id": "DIP3fgB2L2Oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Application de la fonction 'clean_text' à chaque ligne du Dataframe\n",
        "train_df['text_clean1']=train_df['review'].map(lambda X: clean_text(X))\n",
        "train_df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-21T12:44:40.537804Z",
          "iopub.execute_input": "2022-04-21T12:44:40.538075Z",
          "iopub.status.idle": "2022-04-21T12:44:41.056438Z",
          "shell.execute_reply.started": "2022-04-21T12:44:40.538038Z",
          "shell.execute_reply": "2022-04-21T12:44:41.055772Z"
        },
        "trusted": true,
        "id": "RZOmv3OUL2Oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_lens = train_df['text_clean1'].map(lambda x: len(x.split()))\n",
        "n, bins, patches = plt.hist(review_lens, 50, density=True, facecolor='g', alpha=0.75)\n",
        "plt.xlabel('Smarts')\n",
        "plt.ylabel('Probability')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-21T12:44:41.058874Z",
          "iopub.execute_input": "2022-04-21T12:44:41.059300Z",
          "iopub.status.idle": "2022-04-21T12:44:42.130565Z",
          "shell.execute_reply.started": "2022-04-21T12:44:41.059262Z",
          "shell.execute_reply": "2022-04-21T12:44:42.129852Z"
        },
        "trusted": true,
        "id": "fjoNz9HTL2Oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-21T12:44:42.134584Z",
          "iopub.execute_input": "2022-04-21T12:44:42.136572Z",
          "iopub.status.idle": "2022-04-21T12:44:58.863173Z",
          "shell.execute_reply.started": "2022-04-21T12:44:42.136534Z",
          "shell.execute_reply": "2022-04-21T12:44:58.862161Z"
        },
        "trusted": true,
        "id": "YL-F0J7PL2Oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp_fr = spacy.load('en_core_web_sm') # Pour prendre en compte le vocabulaire francais."
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-21T12:44:58.864796Z",
          "iopub.execute_input": "2022-04-21T12:44:58.865129Z",
          "iopub.status.idle": "2022-04-21T12:44:59.478838Z",
          "shell.execute_reply.started": "2022-04-21T12:44:58.865084Z",
          "shell.execute_reply": "2022-04-21T12:44:59.478088Z"
        },
        "trusted": true,
        "id": "J9gUsIE3L2Oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definition de la fonction 'cleanup_text' pour enlever les pronoms personnels et stopwords\n",
        "stopwords = stopwords.words('english')\n",
        "\n",
        "def cleanup_text(docs, logging=False):\n",
        "    \n",
        "    texts = []\n",
        "    doc = nlp_fr(docs)\n",
        "    tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n",
        "    tokens = [tok for tok in tokens if tok not in stopwords]\n",
        "    tokens = ' '.join(tokens)\n",
        "    texts.append(tokens)\n",
        "    return \" \".join(texts)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-21T12:44:59.480051Z",
          "iopub.execute_input": "2022-04-21T12:44:59.480315Z",
          "iopub.status.idle": "2022-04-21T12:44:59.524987Z",
          "shell.execute_reply.started": "2022-04-21T12:44:59.480280Z",
          "shell.execute_reply": "2022-04-21T12:44:59.524050Z"
        },
        "trusted": true,
        "id": "jR-mJDVhL2Oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['text_clean']=train_df['text_clean1'].map(lambda X: cleanup_text(X))\n",
        "train_df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-21T12:45:21.161083Z",
          "iopub.execute_input": "2022-04-21T12:45:21.161924Z"
        },
        "trusted": true,
        "id": "tFFaeO99L2Oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Reviews description with punctuation and stopwords---\\n')\n",
        "print(train_df['text_clean1'][0])\n",
        "print('\\nReviews description after removing  stopwrods---\\n')\n",
        "print(train_df['text_clean'][0])"
      ],
      "metadata": {
        "trusted": true,
        "id": "iMFMyJZkL2Oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Punctuation(string):\n",
        " \n",
        "    # punctuation marks\n",
        "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "    for x in string.lower():\n",
        "        if x in punctuations:\n",
        "            string = string.replace(x, \"\")\n",
        "    print(string)\n",
        "Punctuation(clean_corpus)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-21T12:39:52.550550Z",
          "iopub.execute_input": "2022-04-21T12:39:52.550801Z",
          "iopub.status.idle": "2022-04-21T12:39:52.556723Z",
          "shell.execute_reply.started": "2022-04-21T12:39:52.550774Z",
          "shell.execute_reply": "2022-04-21T12:39:52.556026Z"
        },
        "trusted": true,
        "id": "HpHZfmzkL2Oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "import string\n",
        "\n",
        "punctuations = string.punctuation\n",
        "print('list of punctuations:', punctuations)\n",
        "\n",
        "def punctuation_cleaning(article):\n",
        "    return article.translate(str.maketrans('', '', punctuations))\n",
        "\n",
        "clean_corpus = punctuation_cleaning(train_df['text_clean1'][0])\n",
        "print(clean_corpus)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-21T10:48:48.109105Z",
          "iopub.execute_input": "2022-04-21T10:48:48.109603Z",
          "iopub.status.idle": "2022-04-21T10:48:48.116230Z",
          "shell.execute_reply.started": "2022-04-21T10:48:48.109568Z",
          "shell.execute_reply": "2022-04-21T10:48:48.115146Z"
        },
        "trusted": true,
        "id": "2ftrqBdBL2Ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "tokens = word_tokenize(clean_corpus)\n",
        "print(tokens)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-21T10:51:47.629046Z",
          "iopub.execute_input": "2022-04-21T10:51:47.629807Z",
          "iopub.status.idle": "2022-04-21T10:51:47.649578Z",
          "shell.execute_reply.started": "2022-04-21T10:51:47.629763Z",
          "shell.execute_reply": "2022-04-21T10:51:47.648875Z"
        },
        "trusted": true,
        "id": "R3Y89eezL2Ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-21T10:51:57.788681Z",
          "iopub.execute_input": "2022-04-21T10:51:57.788934Z",
          "iopub.status.idle": "2022-04-21T10:51:57.796844Z",
          "shell.execute_reply.started": "2022-04-21T10:51:57.788907Z",
          "shell.execute_reply": "2022-04-21T10:51:57.795898Z"
        },
        "trusted": true,
        "id": "QA9eABBiL2Ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppression des mots recurrents (stopwords)"
      ],
      "metadata": {
        "id": "dFF6NyUpL2Ok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stopwords are the most common words in any natural language. For the purpose of analyzing text data and building NLP models, these stopwords might not add much value to the meaning of the document. Generally, the most common words used in a text are “the”, “is”, “in”, “for”, “where”, “when”, “to”, “at” etc."
      ],
      "metadata": {
        "id": "TdJwUwViL2Ok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "x_YCzNftL2Ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# recuperation of stopwords in English\n",
        "sw = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "tokens_wsw = []              # initialization of a list\n",
        "for word in tokens:\n",
        "    if word not in sw:\n",
        "        tokens_wsw.append(word)\n",
        "print(tokens_wsw)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-21T12:32:38.481755Z",
          "iopub.execute_input": "2022-04-21T12:32:38.482062Z",
          "iopub.status.idle": "2022-04-21T12:32:38.490038Z",
          "shell.execute_reply.started": "2022-04-21T12:32:38.482029Z",
          "shell.execute_reply": "2022-04-21T12:32:38.489166Z"
        },
        "trusted": true,
        "id": "hADVSFm_L2Ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalisation (Lemmatization)"
      ],
      "metadata": {
        "id": "_QFGpTl4L2Ok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma"
      ],
      "metadata": {
        "id": "jf9AmevpL2Ok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "tokens_wsw=['One','reviewers','mentioned','watching','1','Oz','episode','youll','hooked','They','right','exactly','happened']\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmawords = [lemmatizer.lemmatize(word) for word in tokens_wsw]\n",
        "print (lemmawords)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-21T12:35:09.471272Z",
          "iopub.execute_input": "2022-04-21T12:35:09.471739Z",
          "iopub.status.idle": "2022-04-21T12:35:09.477806Z",
          "shell.execute_reply.started": "2022-04-21T12:35:09.471702Z",
          "shell.execute_reply": "2022-04-21T12:35:09.477109Z"
        },
        "trusted": true,
        "id": "P4C8w_T-L2Ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations de certains mots et leurs nombre d'occurences"
      ],
      "metadata": {
        "id": "oWu8Ua4gL2Ol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A word cloud  is a visual representation of words. Cloud creators are used to highlight popular words and phrases based on frequency and relevance. They provide you with quick and simple visual insights that can lead to more in-depth analyses."
      ],
      "metadata": {
        "id": "ocUbMZeDL2Ol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "# Generate word cloud\n",
        "wordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='salmon', colormap='Pastel1',\n",
        "                      collocations=False, stopwords = STOPWORDS).generate(clean_corpus)\n",
        "# Plot\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)\n",
        "  \n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-21T12:36:22.511660Z",
          "iopub.execute_input": "2022-04-21T12:36:22.511939Z",
          "iopub.status.idle": "2022-04-21T12:36:25.075754Z",
          "shell.execute_reply.started": "2022-04-21T12:36:22.511911Z",
          "shell.execute_reply": "2022-04-21T12:36:25.075077Z"
        },
        "trusted": true,
        "id": "wOC_9MDXL2Ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "Counter(lemmawords).most_common(150)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-21T12:38:43.821220Z",
          "iopub.execute_input": "2022-04-21T12:38:43.821506Z",
          "iopub.status.idle": "2022-04-21T12:38:43.827530Z",
          "shell.execute_reply.started": "2022-04-21T12:38:43.821478Z",
          "shell.execute_reply": "2022-04-21T12:38:43.826593Z"
        },
        "trusted": true,
        "id": "4M3C3aVxL2Ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtention des mots distincts (vocabulaire)"
      ],
      "metadata": {
        "id": "Jluu207yL2Ol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = set(lemmawords)\n",
        "print(vocab)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-21T12:36:29.109936Z",
          "iopub.execute_input": "2022-04-21T12:36:29.110241Z",
          "iopub.status.idle": "2022-04-21T12:36:29.115455Z",
          "shell.execute_reply.started": "2022-04-21T12:36:29.110186Z",
          "shell.execute_reply": "2022-04-21T12:36:29.114210Z"
        },
        "trusted": true,
        "id": "k-ThW0yDL2Ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp = spacy.load('en_core_web_sm')\n",
        "def return_NER(sentence):\n",
        "    # Tokeniser la phrase\n",
        "    doc = sp(sentence)\n",
        "    # Retourner le texte et le label pour chaque entité\n",
        "    return [(X.text, X.label_) for X in doc.ents]\n",
        "\n",
        "return_NER(clean_corpus)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-21T12:36:30.420901Z",
          "iopub.execute_input": "2022-04-21T12:36:30.421453Z",
          "iopub.status.idle": "2022-04-21T12:36:31.032067Z",
          "shell.execute_reply.started": "2022-04-21T12:36:30.421416Z",
          "shell.execute_reply": "2022-04-21T12:36:31.031261Z"
        },
        "trusted": true,
        "id": "kwZnwEtZL2Ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rHcvPmgCL2Ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8wyzcRexL2Ol"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}