{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Prepossessing-project.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AKMADOU/GMM-Group-Project/blob/main/Prepossessing_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4 as bs\n",
        "import urllib\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import re\n",
        "import heapq\n",
        "from string import punctuation"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2022-04-22T00:52:54.142633Z",
          "iopub.execute_input": "2022-04-22T00:52:54.143208Z",
          "iopub.status.idle": "2022-04-22T00:53:04.322342Z",
          "shell.execute_reply.started": "2022-04-22T00:52:54.143165Z",
          "shell.execute_reply": "2022-04-22T00:53:04.321542Z"
        },
        "trusted": true,
        "id": "s37OAfruByOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-22T00:53:40.678448Z",
          "iopub.execute_input": "2022-04-22T00:53:40.678720Z",
          "iopub.status.idle": "2022-04-22T00:53:41.959849Z",
          "shell.execute_reply.started": "2022-04-22T00:53:40.678689Z",
          "shell.execute_reply": "2022-04-22T00:53:41.959061Z"
        },
        "trusted": true,
        "id": "9YALYAFgByOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-22T00:53:43.157354Z",
          "iopub.execute_input": "2022-04-22T00:53:43.158393Z",
          "iopub.status.idle": "2022-04-22T00:53:43.177240Z",
          "shell.execute_reply.started": "2022-04-22T00:53:43.158349Z",
          "shell.execute_reply": "2022-04-22T00:53:43.176573Z"
        },
        "trusted": true,
        "id": "Xzzpv4QsByOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Columns\n",
        "\n",
        "The \"review\" column contains the textual information(input features) and the \"sentiment\" column contains the output labels. The task of any classifier is to correctly predict the \"sentiment\" given any \"review\" or textual column. Hence we have to apply our data cleaning, transformation steps to the \"review\" column."
      ],
      "metadata": {
        "id": "9UAXwMMAByOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Assess the shape of the data\n",
        "print(\"The Shape of the Dataset\".format(),train_df.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-22T00:53:44.795913Z",
          "iopub.execute_input": "2022-04-22T00:53:44.796537Z",
          "iopub.status.idle": "2022-04-22T00:53:44.802331Z",
          "shell.execute_reply.started": "2022-04-22T00:53:44.796498Z",
          "shell.execute_reply": "2022-04-22T00:53:44.801550Z"
        },
        "trusted": true,
        "id": "628GigeCByOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This is the start of the analysis phase where we will first check the amount of data present in either of the sentiments. We will follow this up with some pictorial representations related to the words and frequency mappings."
      ],
      "metadata": {
        "id": "pyWb3Ou1ByOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "good_reviews=train_df[train_df['sentiment']=='positive']['review']\n",
        "bad_reviews=train_df[train_df['sentiment']=='negative']['review']\n",
        "print(\"First 10 samples of good reviews\\n\".format(),good_reviews[:10])\n",
        "print(\"First 10 samples of bad reviews\\n\".format(),bad_reviews[:10])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-22T00:53:45.947570Z",
          "iopub.execute_input": "2022-04-22T00:53:45.948138Z",
          "iopub.status.idle": "2022-04-22T00:53:45.980182Z",
          "shell.execute_reply.started": "2022-04-22T00:53:45.948099Z",
          "shell.execute_reply": "2022-04-22T00:53:45.979494Z"
        },
        "trusted": true,
        "id": "PxwjNyAJByOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Count of good and bad reviews\n",
        "count=train_df['sentiment'].value_counts()\n",
        "print('Total Counts of both sets'.format(),count)\n",
        "\n",
        "print(\"==============\")\n",
        "#Creating a function to plot the counts using matplotlib\n",
        "def plot_counts(count_good,count_bad):\n",
        "    plt.rcParams['figure.figsize']=(6,6)\n",
        "    plt.bar(0,count_good,width=0.6,label='Positive Reviews',color='Green')\n",
        "    plt.legend()\n",
        "    plt.bar(2,count_bad,width=0.6,label='Negative Reviews',color='Red')\n",
        "    plt.legend()\n",
        "    plt.ylabel('Count of Reviews')\n",
        "    plt.xlabel('Types of Reviews')\n",
        "    plt.show()\n",
        "    \n",
        "count_good=train_df[train_df['sentiment']=='positive']\n",
        "count_bad=train_df[train_df['sentiment']=='negative']\n",
        "plot_counts(len(count_good),len(count_bad))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-22T00:53:46.407054Z",
          "iopub.execute_input": "2022-04-22T00:53:46.407277Z",
          "iopub.status.idle": "2022-04-22T00:53:46.845517Z",
          "shell.execute_reply.started": "2022-04-22T00:53:46.407250Z",
          "shell.execute_reply": "2022-04-22T00:53:46.844841Z"
        },
        "trusted": true,
        "id": "Khs6gOSyByOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_df),train_df.index.shape[-1]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-22T00:53:46.903183Z",
          "iopub.execute_input": "2022-04-22T00:53:46.903493Z",
          "iopub.status.idle": "2022-04-22T00:53:46.909360Z",
          "shell.execute_reply.started": "2022-04-22T00:53:46.903464Z",
          "shell.execute_reply": "2022-04-22T00:53:46.908439Z"
        },
        "trusted": true,
        "id": "puNXS4eYByOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Analyse the count of words in each segment- both positive and negative reviews\n",
        "#Function for checking word length\n",
        "def cal_len(data):\n",
        "    return len(data)\n",
        "\n",
        "#Create generic plotter with Seaborn\n",
        "def plot_count(count_ones,count_zeros,title_1,title_2,subtitle):\n",
        "    fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,5))\n",
        "    sns.distplot(count_zeros,ax=ax1,color='Blue')\n",
        "    ax1.set_title(title_1)\n",
        "    sns.distplot(count_ones,ax=ax2,color='Red')\n",
        "    ax2.set_title(title_2)\n",
        "    fig.suptitle(subtitle)\n",
        "    plt.show()    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "count_good_words=count_good['review'].str.split().apply(lambda z:cal_len(z))\n",
        "count_bad_words=count_bad['review'].str.split().apply(lambda z:cal_len(z))\n",
        "print(\"Positive Review Words:\" + str(count_good_words))\n",
        "print(\"Negative Review Words:\" + str(count_bad_words))\n",
        "plot_count(count_good_words,count_bad_words,\"Positive Review\",\"Negative Review\",\"Reviews Word Analysis\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-22T00:53:47.328392Z",
          "iopub.execute_input": "2022-04-22T00:53:47.328643Z",
          "iopub.status.idle": "2022-04-22T00:53:49.539739Z",
          "shell.execute_reply.started": "2022-04-22T00:53:47.328614Z",
          "shell.execute_reply": "2022-04-22T00:53:49.538097Z"
        },
        "trusted": true,
        "id": "RsVzL1GYByOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Analyse Stopwords\n",
        "\n",
        "def plot_count_1(count_ones,count_zeros,title_1,title_2,subtitle):\n",
        "    fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,5))\n",
        "    sns.distplot(count_zeros,ax=ax1,color='Blue')\n",
        "    ax1.set_title(title_1)\n",
        "    sns.distplot(count_ones,ax=ax2,color='Orange')\n",
        "    ax2.set_title(title_2)\n",
        "    fig.suptitle(subtitle)\n",
        "    plt.show()    \n",
        "\n",
        "\n",
        "stops=set(stopwords.words('english'))\n",
        "count_good_stops=count_good['review'].apply(lambda z : np.mean([len(z) for w in str(z).split()]))\n",
        "count_bad_stops=count_bad['review'].apply(lambda z : np.mean([len(z) for w in str(z).split()]))\n",
        "plot_count_1(count_good_stops,count_bad_stops,\"Positive Reviews Stopwords\",\"Negative Reviews Stopwords\",\"Reviews Stopwords Analysis\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-22T00:53:49.541412Z",
          "iopub.execute_input": "2022-04-22T00:53:49.541757Z",
          "iopub.status.idle": "2022-04-22T00:53:53.962557Z",
          "shell.execute_reply.started": "2022-04-22T00:53:49.541717Z",
          "shell.execute_reply": "2022-04-22T00:53:53.961810Z"
        },
        "trusted": true,
        "id": "3Eymk9OlByOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Checking number of Urls\n",
        "count_good_urls=count_good['review'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
        "count_bad_urls=count_bad['review'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
        "\n",
        "plot_count_1(count_good_stops,count_bad_stops,\"Positive Reviews URLs\",\"Negative Reviews URLs\",\"Reviews URLs Analysis\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-22T00:53:53.963890Z",
          "iopub.execute_input": "2022-04-22T00:53:53.964234Z",
          "iopub.status.idle": "2022-04-22T00:53:56.352363Z",
          "shell.execute_reply.started": "2022-04-22T00:53:53.964193Z",
          "shell.execute_reply": "2022-04-22T00:53:56.351680Z"
        },
        "trusted": true,
        "id": "iPD3Pcv9ByOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,46):\n",
        "    article_text = train_df.replace(str([i]), \"\")\n",
        "print(article_text)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-22T00:53:56.354397Z",
          "iopub.execute_input": "2022-04-22T00:53:56.354749Z",
          "iopub.status.idle": "2022-04-22T00:53:56.556032Z",
          "shell.execute_reply.started": "2022-04-22T00:53:56.354708Z",
          "shell.execute_reply": "2022-04-22T00:53:56.555258Z"
        },
        "trusted": true,
        "id": "F4JUvpR9ByOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-22T00:53:56.557233Z",
          "iopub.execute_input": "2022-04-22T00:53:56.557501Z",
          "iopub.status.idle": "2022-04-22T00:54:07.174615Z",
          "shell.execute_reply.started": "2022-04-22T00:53:56.557463Z",
          "shell.execute_reply": "2022-04-22T00:54:07.173649Z"
        },
        "trusted": true,
        "id": "zkC-b5bUByOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    row = re.sub(r\"\\\\\", \" \" ,text, flags=re.DOTALL)\n",
        "    row = re.sub(r\"\\([^()]*\\)\", \" \", row,flags=re.DOTALL) \n",
        "    row = re.sub(r\"\\n\", \" \", row,flags=re.DOTALL)\n",
        "    return row"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-22T00:54:07.178214Z",
          "iopub.execute_input": "2022-04-22T00:54:07.178472Z",
          "iopub.status.idle": "2022-04-22T00:54:07.185240Z",
          "shell.execute_reply.started": "2022-04-22T00:54:07.178439Z",
          "shell.execute_reply": "2022-04-22T00:54:07.184249Z"
        },
        "trusted": true,
        "id": "7xYv1SyhByOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Application de la fonction 'clean_text' à chaque ligne du Dataframe\n",
        "train_df['text_clean1']=train_df['review'].map(lambda X: clean_text(X))\n",
        "train_df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-22T00:54:07.187025Z",
          "iopub.execute_input": "2022-04-22T00:54:07.187397Z",
          "iopub.status.idle": "2022-04-22T00:54:07.695516Z",
          "shell.execute_reply.started": "2022-04-22T00:54:07.187356Z",
          "shell.execute_reply": "2022-04-22T00:54:07.694761Z"
        },
        "trusted": true,
        "id": "wkWrfH4zByOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['review']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-22T00:54:07.696776Z",
          "iopub.execute_input": "2022-04-22T00:54:07.697219Z",
          "iopub.status.idle": "2022-04-22T00:54:07.706776Z",
          "shell.execute_reply.started": "2022-04-22T00:54:07.697177Z",
          "shell.execute_reply": "2022-04-22T00:54:07.705946Z"
        },
        "trusted": true,
        "id": "HZcvXcqfByOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_lens = train_df['text_clean1'].map(lambda x: len(x.split()))\n",
        "n, bins, patches = plt.hist(review_lens, 50, density=True, facecolor='g', alpha=0.75)\n",
        "plt.xlabel('Smarts')\n",
        "plt.ylabel('Probability')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-22T00:54:07.708431Z",
          "iopub.execute_input": "2022-04-22T00:54:07.708813Z",
          "iopub.status.idle": "2022-04-22T00:54:08.626465Z",
          "shell.execute_reply.started": "2022-04-22T00:54:07.708749Z",
          "shell.execute_reply": "2022-04-22T00:54:08.625746Z"
        },
        "trusted": true,
        "id": "qeQJmWYzByOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-22T00:54:08.629673Z",
          "iopub.execute_input": "2022-04-22T00:54:08.629906Z",
          "iopub.status.idle": "2022-04-22T00:54:25.737407Z",
          "shell.execute_reply.started": "2022-04-22T00:54:08.629858Z",
          "shell.execute_reply": "2022-04-22T00:54:25.736498Z"
        },
        "trusted": true,
        "id": "1WL1cbylByOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp_fr = spacy.load('en_core_web_sm') # Pour prendre en compte le vocabulaire francais."
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-22T00:54:25.738993Z",
          "iopub.execute_input": "2022-04-22T00:54:25.739270Z",
          "iopub.status.idle": "2022-04-22T00:54:26.477115Z",
          "shell.execute_reply.started": "2022-04-22T00:54:25.739229Z",
          "shell.execute_reply": "2022-04-22T00:54:26.476297Z"
        },
        "trusted": true,
        "id": "9wTECEtPByOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definition de la fonction 'cleanup_text' pour enlever les pronoms personnels et stopwords\n",
        "stopwords = stopwords.words('english')\n",
        "\n",
        "def cleanup_text(docs, logging=False):\n",
        "    \n",
        "    texts = []\n",
        "    doc = nlp_fr(docs)\n",
        "    tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n",
        "    tokens = [tok for tok in tokens if tok not in stopwords]\n",
        "    tokens = ' '.join(tokens)\n",
        "    texts.append(tokens)\n",
        "    return \" \".join(texts)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-22T00:54:26.478467Z",
          "iopub.execute_input": "2022-04-22T00:54:26.478727Z",
          "iopub.status.idle": "2022-04-22T00:54:26.486638Z",
          "shell.execute_reply.started": "2022-04-22T00:54:26.478693Z",
          "shell.execute_reply": "2022-04-22T00:54:26.485950Z"
        },
        "trusted": true,
        "id": "CMeH1sUwByOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# s = pd.Series(train_df['text_clean1'], name=\"vals\")\n",
        "# df = s.to_frame(name=\"Reviews\")\n",
        "# for index, row in df.iterrows(): \n",
        "#     Reviews=print(row['Reviews'] )"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-22T00:54:26.488116Z",
          "iopub.execute_input": "2022-04-22T00:54:26.488513Z",
          "iopub.status.idle": "2022-04-22T00:54:26.495658Z",
          "shell.execute_reply.started": "2022-04-22T00:54:26.488475Z",
          "shell.execute_reply": "2022-04-22T00:54:26.494933Z"
        },
        "trusted": true,
        "id": "pCHpbIcWByOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Punctuation(string):\n",
        " \n",
        "    # punctuation marks\n",
        "    punctuations = '''!()-[]{};:'\"\\,,<>./?@#$%^&*_~br'''\n",
        "    for x in string.lower():\n",
        "        if x in punctuations:\n",
        "            string = string.replace(x, \"\")\n",
        "    return string\n",
        "#     print(string)\n",
        "\n",
        "# Punctuation(train_df['text_clean1'])\n",
        "train_df['text_clean1'].apply(Punctuation)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-22T00:54:26.497005Z",
          "iopub.execute_input": "2022-04-22T00:54:26.497825Z",
          "iopub.status.idle": "2022-04-22T00:54:41.650642Z",
          "shell.execute_reply.started": "2022-04-22T00:54:26.497787Z",
          "shell.execute_reply": "2022-04-22T00:54:41.649823Z"
        },
        "trusted": true,
        "id": "-4GbM127ByOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lowercase(intext):\n",
        "    return intext.lower()    \n",
        "\n",
        "article_text_l = train_df['text_clean1'].apply(lowercase)\n",
        "article_text_l.apply(Punctuation)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-22T00:54:41.652131Z",
          "iopub.execute_input": "2022-04-22T00:54:41.652403Z",
          "iopub.status.idle": "2022-04-22T00:54:56.144508Z",
          "shell.execute_reply.started": "2022-04-22T00:54:41.652366Z",
          "shell.execute_reply": "2022-04-22T00:54:56.143749Z"
        },
        "trusted": true,
        "id": "ObUkjx3wByOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from string import punctuation\n",
        "# import string\n",
        "\n",
        "# punctuations = string.punctuation\n",
        "# print('list of punctuations:', punctuations)\n",
        "\n",
        "# def punctuation_cleaning(article):\n",
        "#     return article.translate(str.maketrans('', '', punctuations))\n",
        "\n",
        "# clean_corpus = punctuation_cleaning(Reviews)\n",
        "# print(clean_corpus)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-22T00:54:56.145744Z",
          "iopub.execute_input": "2022-04-22T00:54:56.146029Z",
          "iopub.status.idle": "2022-04-22T00:54:56.150463Z",
          "shell.execute_reply.started": "2022-04-22T00:54:56.145992Z",
          "shell.execute_reply": "2022-04-22T00:54:56.149738Z"
        },
        "trusted": true,
        "id": "3O2NxNz6ByOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "#   tokens = word_tokenize(string)\n",
        "# print(tokens)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-22T00:54:56.151562Z",
          "iopub.execute_input": "2022-04-22T00:54:56.154206Z",
          "iopub.status.idle": "2022-04-22T00:54:56.161266Z",
          "shell.execute_reply.started": "2022-04-22T00:54:56.154166Z",
          "shell.execute_reply": "2022-04-22T00:54:56.160531Z"
        },
        "trusted": true,
        "id": "WObGvJ-jByOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(string):\n",
        "    return word_tokenize(string)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-22T00:54:56.162507Z",
          "iopub.execute_input": "2022-04-22T00:54:56.163028Z",
          "iopub.status.idle": "2022-04-22T00:54:56.172461Z",
          "shell.execute_reply.started": "2022-04-22T00:54:56.162982Z",
          "shell.execute_reply": "2022-04-22T00:54:56.171654Z"
        },
        "trusted": true,
        "id": "jiqwbel3ByOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(article_text_l.apply(Punctuation)).apply(tokenize)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-22T00:54:56.173977Z",
          "iopub.execute_input": "2022-04-22T00:54:56.174304Z",
          "iopub.status.idle": "2022-04-22T00:56:27.757468Z",
          "shell.execute_reply.started": "2022-04-22T00:54:56.174262Z",
          "shell.execute_reply": "2022-04-22T00:56:27.756743Z"
        },
        "trusted": true,
        "id": "QlgJzkskByOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frame = { 'Document': (article_text_l.apply(Punctuation)).apply(tokenize), 'Sentiment': train_df['sentiment']}\n",
        "Tokenized_data = pd.DataFrame(frame)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-22T00:56:27.758942Z",
          "iopub.execute_input": "2022-04-22T00:56:27.759198Z",
          "iopub.status.idle": "2022-04-22T00:57:58.069706Z",
          "shell.execute_reply.started": "2022-04-22T00:56:27.759162Z",
          "shell.execute_reply": "2022-04-22T00:57:58.068774Z"
        },
        "trusted": true,
        "id": "aZHvwIF3ByOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tokenized_data.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-22T00:57:58.070931Z",
          "iopub.execute_input": "2022-04-22T00:57:58.071176Z",
          "iopub.status.idle": "2022-04-22T00:57:58.084593Z",
          "shell.execute_reply.started": "2022-04-22T00:57:58.071141Z",
          "shell.execute_reply": "2022-04-22T00:57:58.083902Z"
        },
        "trusted": true,
        "id": "D6uiT6LAByOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppression des mots recurrents (stopwords)"
      ],
      "metadata": {
        "id": "oKrS73nEByOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Tokenized_data['Sentiment']=Tokenized_data['Sentiment'].apply(lambda x:1 if x=='positive' else 0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-22T00:57:58.085921Z",
          "iopub.execute_input": "2022-04-22T00:57:58.086500Z",
          "iopub.status.idle": "2022-04-22T00:57:58.124179Z",
          "shell.execute_reply.started": "2022-04-22T00:57:58.086437Z",
          "shell.execute_reply": "2022-04-22T00:57:58.123541Z"
        },
        "trusted": true,
        "id": "77JHl7GpByOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tokenized_data"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-22T00:57:58.125470Z",
          "iopub.execute_input": "2022-04-22T00:57:58.125953Z",
          "iopub.status.idle": "2022-04-22T00:57:58.146329Z",
          "shell.execute_reply.started": "2022-04-22T00:57:58.125911Z",
          "shell.execute_reply": "2022-04-22T00:57:58.145670Z"
        },
        "trusted": true,
        "id": "hHEVVg1uByOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stopwords are the most common words in any natural language. For the purpose of analyzing text data and building NLP models, these stopwords might not add much value to the meaning of the document. Generally, the most common words used in a text are “the”, “is”, “in”, “for”, “where”, “when”, “to”, “at” etc."
      ],
      "metadata": {
        "id": "T5Pwe6ZQByOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "def stopword(string):\n",
        "    # recuperation of stopwords in English\n",
        "    sw = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "    tokens_wsw = []              # initialization of a list\n",
        "    for word in string:\n",
        "        if word not in sw:\n",
        "            tokens_wsw.append(word)\n",
        "    return  tokens_wsw  \n",
        "Tokenized_data['Document'].apply(stopword)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-22T00:57:58.147585Z",
          "iopub.execute_input": "2022-04-22T00:57:58.147825Z",
          "iopub.status.idle": "2022-04-22T00:58:21.257130Z",
          "shell.execute_reply.started": "2022-04-22T00:57:58.147792Z",
          "shell.execute_reply": "2022-04-22T00:58:21.256247Z"
        },
        "trusted": true,
        "id": "IA17OvWGByOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tokenized_data['Document'].apply(lambda x: set(x))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-22T00:58:21.262071Z",
          "iopub.execute_input": "2022-04-22T00:58:21.264129Z",
          "iopub.status.idle": "2022-04-22T00:58:24.056507Z",
          "shell.execute_reply.started": "2022-04-22T00:58:21.264086Z",
          "shell.execute_reply": "2022-04-22T00:58:24.055778Z"
        },
        "trusted": true,
        "id": "iUi5XBKUByOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tokenized_data.to_csv(\"Tokenize.csv\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-22T00:58:24.057674Z",
          "iopub.execute_input": "2022-04-22T00:58:24.057954Z",
          "iopub.status.idle": "2022-04-22T00:58:28.266409Z",
          "shell.execute_reply.started": "2022-04-22T00:58:24.057906Z",
          "shell.execute_reply": "2022-04-22T00:58:28.265313Z"
        },
        "trusted": true,
        "id": "az4CNVEhByOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalisation (Lemmatization)"
      ],
      "metadata": {
        "id": "E3LB1pmCByOx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma"
      ],
      "metadata": {
        "id": "zuwQiAzhByOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "# tokens_wsw=['One','reviewers','mentioned','watching','1','Oz','episode','youll','hooked','They','right','exactly','happened']\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmawords = Tokenized_data['Document'].apply(lambda x: [lemmatizer.lemmatize(i) for i in x])#[lemmatizer.lemmatize(word) for word in Tokenized_data['Document'].ap]\n",
        "lemmawords"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-22T00:58:28.269605Z",
          "iopub.execute_input": "2022-04-22T00:58:28.269908Z",
          "iopub.status.idle": "2022-04-22T00:59:16.711395Z",
          "shell.execute_reply.started": "2022-04-22T00:58:28.269855Z",
          "shell.execute_reply": "2022-04-22T00:59:16.710667Z"
        },
        "trusted": true,
        "id": "sboEdlRQByOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmawords.apply(lambda x: set(x))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-22T00:59:16.714979Z",
          "iopub.execute_input": "2022-04-22T00:59:16.715183Z",
          "iopub.status.idle": "2022-04-22T00:59:17.923447Z",
          "shell.execute_reply.started": "2022-04-22T00:59:16.715157Z",
          "shell.execute_reply": "2022-04-22T00:59:17.922695Z"
        },
        "trusted": true,
        "id": "JWCvgWkEByOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations de certains mots et leurs nombre d'occurences"
      ],
      "metadata": {
        "id": "MI2RVztSByOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frame = { 'Document':lemmawords.apply(lambda x: set(x)), 'Sentiment': train_df['sentiment']}\n",
        "lemmawords = pd.DataFrame(frame)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-22T00:59:17.924734Z",
          "iopub.execute_input": "2022-04-22T00:59:17.925392Z",
          "iopub.status.idle": "2022-04-22T00:59:21.293359Z",
          "shell.execute_reply.started": "2022-04-22T00:59:17.925349Z",
          "shell.execute_reply": "2022-04-22T00:59:21.292568Z"
        },
        "trusted": true,
        "id": "jTbsNcXZByOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmawords"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-04-22T01:02:32.379516Z",
          "iopub.execute_input": "2022-04-22T01:02:32.380389Z",
          "iopub.status.idle": "2022-04-22T01:02:32.401534Z",
          "shell.execute_reply.started": "2022-04-22T01:02:32.380340Z",
          "shell.execute_reply": "2022-04-22T01:02:32.400757Z"
        },
        "trusted": true,
        "id": "pvoz4bwQByOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A word cloud  is a visual representation of words. Cloud creators are used to highlight popular words and phrases based on frequency and relevance. They provide you with quick and simple visual insights that can lead to more in-depth analyses."
      ],
      "metadata": {
        "id": "gCR0JFYrByOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "# Generate word cloud\n",
        "wordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='salmon', colormap='Pastel1',\n",
        "                      collocations=False, stopwords = STOPWORDS).generate(clean_corpus)\n",
        "# Plot\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)\n",
        "  \n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "4QppK1NtByOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "Counter(lemmawords).most_common(1000)"
      ],
      "metadata": {
        "trusted": true,
        "id": "TP6vJ2qpByOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtention des mots distincts (vocabulaire)"
      ],
      "metadata": {
        "id": "pyHr4iC7ByOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = set(lemmawords)\n",
        "print(vocab)"
      ],
      "metadata": {
        "trusted": true,
        "id": "HhRPzpA9ByOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp = spacy.load('en_core_web_sm')\n",
        "def return_NER(sentence):\n",
        "    # Tokeniser la phrase\n",
        "    doc = sp(sentence)\n",
        "    # Retourner le texte et le label pour chaque entité\n",
        "    return [(X.text, X.label_) for X in doc.ents]\n",
        "\n",
        "return_NER(clean_corpus)"
      ],
      "metadata": {
        "trusted": true,
        "id": "TCkFGEJSByOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PMl5x0XBByOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IcGFEmrSByOy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}